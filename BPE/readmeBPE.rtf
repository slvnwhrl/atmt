{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ### Step 1 ###\
Learn and apply BPE to data in baseline/preprocessed_data.\
Apply preprocessing-script to this newly created data (stored in BPE/preprocessed_data).\
\'97> Command: 
\f1\fs22 \cf2 \CocoaLigature0 bash BPE_add_preprocessing.sh
\f0\fs24 \cf0 \CocoaLigature1 \
\
### Step 2 ###\
Run training on data in BPE/prepared_data.\
\'97> Command: 
\f1\fs22 \cf2 \CocoaLigature0 python train.py --data BPE/prepared_data --save-dir BPE/checkpoints\
\

\f0\fs24 ### Step 3 ###\
Translate.\
\'97> Command:
\f1\fs22  python translate.py --data BPE/prepared_data --checkpoint-path BPE/checkpoints/checkpoint_best.pt --output BPE/model_BPE_translations_seg.txt\

\f0\fs24 \cf0 \CocoaLigature1 \
### Step 3 ###\
Restore segmentation.\
\'97> Command on translation-textfile: sed -E 's/(@@ )|(@@ ?$)//g' 
\f1\fs22 \cf2 \CocoaLigature0 BPE/model_BPE_translations_seg.txt > BPE/model_BPE_translations.txt
\f0\fs24 \cf0 \CocoaLigature1 \
\
### Step 4 ###\
Postprocess translations.\
\'97> Command: 
\f1\fs22 \cf2 \CocoaLigature0 bash postprocess.sh BPE/model_BPE_translations.txt BPE/model_BPE_translations.out en
\f0\fs24 \cf0 \CocoaLigature1 \
\
### Step 5 ###\
Calculate BLEU score.\
\'97> Command: 
\f1\fs22 \cf2 \CocoaLigature0 cat BPE/model_BPE_translations.out | sacrebleu baseline/raw_data/test.en\
\
Note: Results for different setups are stored in seperate folders after this procedure.
\f0\fs24 \cf0 \CocoaLigature1 \
\
Link word segmentation: https://github.com/rsennrich/subword-nmt\
}